#!/bin/bash
set -euo pipefail

# -- Logging function with timestamps, logs to both console and file --
LOG_FILE="/var/log/k3s-graceful-shutdown.log"
log() {
  local ts
  ts=$(date "+%Y-%m-%d %H:%M:%S")
  echo "[$ts] $*" | tee -a "$LOG_FILE"
}

export KUBECONFIG=/home/k3s/.kube/config
ACTION="${1:-}"
NODE_NAME=$(hostname)
LOCK_NAMESPACE="kube-system"
LOCK_NAME="shutdown-lock"
LOCK_SSH_PREFIX="ssh-lock-"
TODAY_UTC="$(date -u +%F)"
KUBECTL="/usr/bin/kubectl"

log "====================[ SHUTDOWN START ]===================="
log "  Node: $NODE_NAME  |  Role: {{ role }}"

# Argument actions received by systemd argument
# Action: remove-ssh-lock
if [[ "$ACTION" == "remove-ssh-lock" ]]; then
  CM_NAME="${LOCK_SSH_PREFIX}${NODE_NAME}"
  log "[ACTION] remove-ssh-lock requested for $NODE_NAME -> $CM_NAME"
  if $KUBECTL -n "$LOCK_NAMESPACE" get configmap "$CM_NAME" >/dev/null 2>&1; then
    $KUBECTL -n "$LOCK_NAMESPACE" delete configmap "$CM_NAME" --ignore-not-found >/dev/null 2>&1 || true
    log "[ACTION] Deleted SSH lock ConfigMap $CM_NAME"
  else
    log "[ACTION] No SSH lock ConfigMap $CM_NAME present; proceeding"
  fi
fi

{% if role == "control-plane" %}
log "[WAIT] for workers actions..."
sleep 15
{% endif %}

{% if role == "worker" %}
# Wait 15 seconds for worker nodes
log "[INFO] Worker node detected, waiting 15 seconds before proceeding..."
sleep 15
{% endif %}

# SSH lock precheck
log "[PRECHECK] Checking for active SSH locks..."

# Fetch all CMs once
LOCKS_JSON="$($KUBECTL -n "$LOCK_NAMESPACE" get configmap -o json 2>/dev/null || echo '{"items": []}')"

# Delete stale locks
echo "$LOCKS_JSON" \
  | jq -r --arg p "$LOCK_SSH_PREFIX" --arg today "$TODAY_UTC" '
      .items[]
      | select(.metadata.name | startswith($p))
      | {name: .metadata.name, created: (.metadata.creationTimestamp // "" | split("T")[0])}
      | select(.created != "" and .created != $today)
      | .name' \
  | while read -r cm; do
      [[ -z "$cm" ]] && continue
      log "[PRECHECK] Removing stale SSH lock ConfigMap $cm"
      $KUBECTL -n "$LOCK_NAMESPACE" delete configmap "$cm" --ignore-not-found || true
    done

# Recompute after potential deletions
LOCKS_JSON="$($KUBECTL -n "$LOCK_NAMESPACE" get configmap -o json 2>/dev/null || echo '{"items": []}')"

# Any valid locks?
VALID_LOCKS="$(echo "$LOCKS_JSON" \
  | jq -r --arg p "$LOCK_SSH_PREFIX" --arg today "$TODAY_UTC" '
      .items[]
      | select(.metadata.name | startswith($p))
      | select((.metadata.creationTimestamp // "" | split("T")[0]) == $today)
      | .metadata.name')"

if [[ -n "$VALID_LOCKS" ]]; then
  log "[ABORT] Active SSH lock(s) detected for today (UTC): $(echo "$VALID_LOCKS" | tr '\n' ' ')"
  log "[ABORT] Skipping shutdown to avoid spooking the animals"
  exit 0
fi
log "[PRECHECK] No active SSH locks, proceeding with shutdown"

{% if role == "control-plane" %}
# Function: Scale down all Deployments and StatefulSets for a given label selector
scale_down_workloads() {
  local label="$1"
  local desc="$2"

  log "[SHUTDOWN] Scaling down workloads with label '$label' ($desc)..."

  # Get all matching Deployments and StatefulSets in one query
  $KUBECTL get deploy,sts --all-namespaces -l "$label" -o jsonpath='{range .items[*]}{.kind}{";"}{.metadata.namespace}{";"}{.metadata.name}{"\n"}{end}' |
  while IFS=";" read -r kind ns name; do
    log "  Scaling $kind $name in namespace $ns to 0 replicas"
    $KUBECTL -n "$ns" scale "$kind" "$name" --replicas=0
  done
}

# Create shutdown lock ConfigMap and scale down workloads ---
log "[SHUTDOWN] Creating ConfigMap $LOCK_NAME in namespace $LOCK_NAMESPACE..."
$KUBECTL -n "$LOCK_NAMESPACE" create configmap "$LOCK_NAME" --from-literal=initiator="$NODE_NAME" --dry-run=client -o yaml | $KUBECTL apply -f -

# Scale down ArgoCD & Prometheus operator
scale_down_workloads "app.kubernetes.io/name=argocd-application-controller" "ArgoCD"
scale_down_workloads "app.kubernetes.io/name=kube-prometheus-stack-prometheus-operator" "Prometheus"
sleep 10

# Scale down storage dependent workloads
scale_down_workloads "storage=rook-ceph,shutdown=app" "Haystack apps"
sleep 10
scale_down_workloads "storage=rook-ceph,shutdown=db" "Haystack databases"

# Hibernating the CNPG PostgreSQL cluster
log "[SHUTDOWN] Locating CNPG cluster (autodetecting namespace and cluster name)..."

CNPG_CLUSTER_INFO="$($KUBECTL get cluster.postgresql.cnpg.io --all-namespaces -o json | jq -r '.items[0] | "\(.metadata.namespace) \(.metadata.name)"')"
CNPG_NAMESPACE="$(awk '{print $1}' <<< "$CNPG_CLUSTER_INFO")"
CNPG_CLUSTER_NAME="$(awk '{print $2}' <<< "$CNPG_CLUSTER_INFO")"

if [[ -z "$CNPG_NAMESPACE" ]] || [[ -z "$CNPG_CLUSTER_NAME" ]]; then
  log "[WARN] No CNPG cluster found. Cluster may already be hibernated or not running. Skipping hibernation step."
else
  log "[SHUTDOWN] Annotating CNPG cluster '$CNPG_CLUSTER_NAME' in namespace '$CNPG_NAMESPACE' for hibernation..."
  $KUBECTL -n "$CNPG_NAMESPACE" annotate cluster.postgresql.cnpg.io "$CNPG_CLUSTER_NAME" --overwrite cnpg.io/hibernation=on

  log "[WAIT] for CNPG pods to enter and finish Terminating state..."
  while true; do
    TERMINATING_COUNT=$($KUBECTL -n "$CNPG_NAMESPACE" get pods -l cnpg.io/cluster="$CNPG_CLUSTER_NAME" -o json | jq '[.items[] | select(.metadata.deletionTimestamp != null)] | length')
    if [[ "$TERMINATING_COUNT" -eq 0 ]]; then
      log "[SHUTDOWN] CNPG cluster pods have terminated"
      break
    fi
    log "  ...still waiting for CNPG pods to terminate"
    sleep 15
  done
fi

log "[SHUTDOWN] All pods dependent on storage have been terminated"
log "[WAIT] for all Rook-Ceph volumes to be detached from all nodes..."

log "[SHUTDOWN] Removing shutdown lock ConfigMap $LOCK_NAME in namespace $LOCK_NAMESPACE..."
$KUBECTL -n "$LOCK_NAMESPACE" delete configmap "$LOCK_NAME" --ignore-not-found
{% endif %}

{% if role == "worker" %}
# --- WORKER NODE: Wait if control-plane lock exists, then cordon, drain, shutdown ---
log "[SHUTDOWN] Checking for shutdown lock ConfigMap..."

while $KUBECTL -n "$LOCK_NAMESPACE" get configmap "$LOCK_NAME" >/dev/null 2>&1; do
  log "[WAIT] for lock ConfigMap $LOCK_NAME exists"
  sleep 10
done

log "[SHUTDOWN] No shutdown lock found, proceeding..."
{% endif %}

{% if role == "control-plane" %}
# --- CONTROL-PLANE NODE: Wait for all others to be gone ---
log "[WAIT] for all workers to shutdown first..."
while true; do
  READY_NODES=$($KUBECTL get nodes -o json | jq -r --arg NODE "$NODE_NAME" \
    '.items[] | select(.metadata.name != $NODE) | select((.spec.unschedulable != true) and (.status.conditions[] | select(.type=="Ready" and .status=="True"))) | .metadata.name')
  if [[ -z "$READY_NODES" ]]; then
    log "[SHUTDOWN] All worker nodes are shutdown or NotReady. Proceeding..."
    break
  else
    log "[WAIT] for nodes: $READY_NODES"
    sleep 10
  fi
done

{% endif %}
log "[SHUTDOWN] Cordoning $NODE_NAME"
$KUBECTL cordon "$NODE_NAME" || true

log "[SHUTDOWN] Draining $NODE_NAME"
$KUBECTL drain "$NODE_NAME" \
  --ignore-daemonsets \
  --force \
  --delete-emptydir-data \
  --disable-eviction=true || true

log "[SHUTDOWN] Tainting node to block early workload startup"
$KUBECTL taint nodes "$NODE_NAME" dns-unready=true:NoExecute --overwrite || true

log "[WAIT] for the node to drain before stopping the K3s service"
sleep 15

{% if role == "control-plane" %}
log "[SHUTDOWN] Stopping k3s (control-plane)"
if systemctl is-active --quiet k3s; then
  sudo systemctl stop k3s
else
  log "[SHUTDOWN] k3s service already stopped or not present."
fi
{% endif %}
{% if role == "worker" %}
log "[SHUTDOWN] Stopping k3s-agent (worker)"
if systemctl is-active --quiet k3s-agent; then
  sudo systemctl stop k3s-agent
else
  log "[SHUTDOWN] k3s-agent service already stopped or not present."
fi
{% endif %}

log "[SHUTDOWN] Powering off node"
log "===================[ SHUTDOWN COMPLETE ]==================="
sudo shutdown -h now
exit 0